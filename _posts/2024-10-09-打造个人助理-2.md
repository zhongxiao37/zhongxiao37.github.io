---
layout: default
title: 打造个人助理-2
date: 2024-10-09 22:06 +0800
categories: bert
---

借着上次的机会，自己打造了一个个人版的小爱同学，自己对 Bert 也感了兴趣。

1. 为什么是 Bert，而不用 LLM？
2. Bert 和 LLM 都是基于 Transformer，那什么是 Transformer？

![Bert Tasks](bert_tasks.png)

## NLP 的历史

每个人对于 Transformer 和 Bert 的解读都不一样，3Blue1Brown 居然都还有视频来解释[什么是 transformer](https://www.bilibili.com/video/BV1qM4m1d7is/)。但如果是现在去回顾历史，就会发现这些东西的演变真的很有意思。

要介绍 Transformer，就不得不提到 NLP 的发展过程。

最初的时候，为了解决机器翻译的问题，人们提出了 seq2seq 模型，即翻译一段句子。到了 2014 年，两篇论文[1](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)[2](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)介绍了基于神经网络，实现的机器翻译。

encoder 和 decoder 的组合，往往都是 RNN 循环神经网络，其大致实现如下:

1. 每次读入一个单词，embedding 之后通过 encoder 转换成为上下文（可以认为是理解了整个句子或者 Hidden state），直到读完整个句子。
2. 传入最后生成的 hidden state，再通过 decoder 翻译成另外一个语言，每次蹦一个词出来，依次循环。

基于 RNN 的机器翻译，中间的 context 上下文非常关键，但也成了瓶颈，尤其是在长句子的理解上。2014 年的另外两篇论文[3](https://arxiv.org/abs/1409.0473)[4](https://arxiv.org/abs/1508.04025)提出了注意力机制，有效地让模型更加关注每个单词。
首先，hidden state 不再是最后一层，而是针对每个单词都有一个 hidden state。
其次，针对每个 hidden state，进行评分和 softmax，进而计算出相应输出。

2017 年，一篇[Attention is All you Need](https://arxiv.org/abs/1706.03762)的论文横空出世，Transformer 诞生了。上面基于 RNN 的模型，效果是不错了，但是训练速度却很慢。

## Reference

1. [https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
